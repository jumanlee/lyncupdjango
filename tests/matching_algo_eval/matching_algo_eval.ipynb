{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "628b7cc8-b3ff-4d7c-a8ba-f9a083bb2b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>226</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813731</th>\n",
       "      <td>73515</td>\n",
       "      <td>14345</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813732</th>\n",
       "      <td>73515</td>\n",
       "      <td>16512</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813733</th>\n",
       "      <td>73515</td>\n",
       "      <td>17187</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813734</th>\n",
       "      <td>73515</td>\n",
       "      <td>22145</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813735</th>\n",
       "      <td>73516</td>\n",
       "      <td>790</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7813736 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  anime_id  rating\n",
       "0              1        20      -1\n",
       "1              1        24      -1\n",
       "2              1        79      -1\n",
       "3              1       226      -1\n",
       "4              1       241      -1\n",
       "...          ...       ...     ...\n",
       "7813731    73515     14345       7\n",
       "7813732    73515     16512       7\n",
       "7813733    73515     17187       9\n",
       "7813734    73515     22145      10\n",
       "7813735    73516       790       9\n",
       "\n",
       "[7813736 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data processing\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "#load the ratings_df data and show details\n",
    "ratings_df = pd.read_csv(\"animerating.csv\")\n",
    "ratings_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2cc967-545f-469a-a06a-b9812c17421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6026\n",
      "7540\n",
      "Total ratings after sampling: 625098\n",
      "Total liked ratings: 490644\n",
      "num_test_movies: 6068\n",
      "num_train_movies: 6575\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#function to hold out a fraction of data for testing. This is for train/test split. \n",
    "def holdout_liked_movies(ratings_df, holdout_frac=0.3, random_state=42):\n",
    "    #get all data that has a movie rating of 5 or higher, out of 10 \n",
    "    #(movies that a user would find acceptable enough to want to rate)\n",
    "    liked_df = ratings_df[ratings_df[\"rating\"] >= 5.0]\n",
    "    print(\"Total liked ratings:\", len(liked_df))\n",
    "\n",
    "    train_list, test_list = [], []\n",
    "\n",
    "    for user in liked_df[\"userId\"].unique():\n",
    "        user_likes = liked_df[liked_df['userId'] == user]\n",
    "        if len(user_likes) >= 2:\n",
    "            test_sample = user_likes.sample(frac=holdout_frac, random_state=random_state)\n",
    "            train_sample = user_likes.drop(test_sample.index)\n",
    "            train_list.append(train_sample)\n",
    "            test_list.append(test_sample)\n",
    "        else:\n",
    "            #should not split if only one interaction\n",
    "            train_list.append(user_likes)  \n",
    "\n",
    "    train_liked_df = pd.concat(train_list)\n",
    "    if len(test_list) > 0:\n",
    "        test_liked_df = pd.concat(test_list)\n",
    "    else:\n",
    "        test_liked_df = pd.DataFrame()\n",
    "\n",
    "    #counts how many times each unique movieId appears.\n",
    "    #overall_counts becomes a pandas Series where The index = each unique movieId and \n",
    "    #value = how many times that movie appears in the data\n",
    "    overall_counts = liked_df['movieId'].value_counts()\n",
    "    \n",
    "    #movies_to_fix is a list of movie id's that appear more than once across the train and test data splits combined\n",
    "    movies_to_fix = []\n",
    "    for movie_id, count in overall_counts.items():\n",
    "    #if the movie appears more than once, add it to the list\n",
    "        if count > 1:\n",
    "            movies_to_fix.append(movie_id)\n",
    "\n",
    "    #make sure that every movie in movies_to_fix exists in both train_liked_df and test_liked_df\n",
    "    #this is important to ensure that the test is fairly evaluated.\n",
    "    for movie in movies_to_fix:\n",
    "        #ensure the presence in each split\n",
    "        #.any() is a method that checks if at least one value is True in a series\n",
    "        in_train = train_liked_df[\"movieId\"] == movie\n",
    "        in_test = test_liked_df['movieId'] == movie\n",
    "        has_in_train = in_train.any()\n",
    "        has_in_test = in_test.any()\n",
    "\n",
    "        if not has_in_train and has_in_test:\n",
    "            #movie only in test: move one instance from test to train.\n",
    "            movie_sample = test_liked_df[test_liked_df[\"movieId\"] == movie].sample(n=1, random_state=random_state)\n",
    "            #use .index to identify which row in the Dataframe to drop, remember .index gets the position of the row\n",
    "            test_liked_df = test_liked_df.drop(movie_sample.index)\n",
    "            train_liked_df = pd.concat([train_liked_df, movie_sample])\n",
    "        elif has_in_train and not has_in_test:\n",
    "            #movie only in train: move one instance from train to test.\n",
    "            movie_sample = train_liked_df[train_liked_df['movieId'] == movie].sample(n=1, random_state=random_state)\n",
    "            #use .index to identify which row in the Dataframe to drop, remember .index gets the position of the row\n",
    "            train_liked_df = train_liked_df.drop(movie_sample.index)\n",
    "            test_liked_df = pd.concat([test_liked_df, movie_sample])\n",
    "\n",
    "    return train_liked_df, test_liked_df\n",
    "\n",
    "#get data from csv\n",
    "ratings_df = pd.read_csv(\"animerating.csv\")\n",
    "#rename column for easier reading\n",
    "ratings_df = ratings_df.rename(columns={\"anime_id\": \"movieId\", \"user_id\": \"userId\"})\n",
    "#extract a portion for training and testing, \n",
    "#as training the entire dataset (more than 7 million) with CPU would require too much computation and time  \n",
    "ratings_df = ratings_df.iloc[:int(len(ratings_df) * 0.08)]\n",
    "num_unique_users = ratings_df['userId'].nunique()\n",
    "num_unique_movies = ratings_df['movieId'].nunique()\n",
    "print(num_unique_users)\n",
    "print(num_unique_movies)\n",
    "print(\"Total ratings after sampling:\", len(ratings_df))\n",
    "\n",
    "#holdout liked movies for train/test split\n",
    "train_liked_df, test_liked_df = holdout_liked_movies(ratings_df, holdout_frac=0.3)\n",
    "\n",
    "num_test_movies = test_liked_df['movieId'].nunique()\n",
    "num_train_movies = train_liked_df['movieId'].nunique()\n",
    "print(f\"num_test_movies: {num_test_movies}\")\n",
    "print(f\"num_train_movies: {num_train_movies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eaf384e-6b92-40cc-9dd2-59269d4230dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|█| 12258/12258 [02:20<00:00, 86.96it/s]\n",
      "Generating walks (CPU: 1): 100%|████████████████| 80/80 [23:56<00:00, 17.96s/it]\n"
     ]
    }
   ],
   "source": [
    "#directed graph construction\n",
    "#directed edge from user -> movie with weight = rating value.\n",
    "#reverse edge from movie -> user with weight = reverse_weight.\n",
    "def create_directed_bipartite_graph_from_ratings(ratings_df, reverse_weight=1.0):\n",
    "    graph = nx.DiGraph()\n",
    "    #add user nodes.\n",
    "    for user in ratings_df['userId'].unique():\n",
    "        user_node = f\"u_{user}\"\n",
    "        graph.add_node(user_node, bipartite=0, node_type=\"user\")\n",
    "    #add movie nodes.\n",
    "    for movie in ratings_df[\"movieId\"].unique():\n",
    "        movie_node = f\"m_{movie}\"\n",
    "        graph.add_node(movie_node, bipartite=1, node_type='movie')\n",
    "    #add directed edges.\n",
    "    for i, row in ratings_df.iterrows():\n",
    "        user_node = f\"u_{row['userId']}\"\n",
    "        movie_node = f\"m_{row['movieId']}\"\n",
    "        rating = float(row['rating'])\n",
    "        \n",
    "        #add edge from user to movie with rating value\n",
    "        if graph.has_edge(user_node, movie_node):\n",
    "            graph[user_node][movie_node][\"weight\"] = 1.0\n",
    "        else:\n",
    "            graph.add_edge(user_node, movie_node, weight=1.0)\n",
    "        \n",
    "        #add reverse edge from movie to user with a fixed weight.\n",
    "        if graph.has_edge(movie_node, user_node):\n",
    "            graph[movie_node][user_node]['weight'] = 0.5\n",
    "        else:\n",
    "            graph.add_edge(movie_node, user_node, weight=1.0)\n",
    "    return graph\n",
    "\n",
    "#create annoy file from ratings\n",
    "def create_node2vec_annoy_from_ratings(ratings_df, base_dir=\"Annoy\", embed_dimensions=256, num_trees=50):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    #create a directed bipartite graph\n",
    "    graph = create_directed_bipartite_graph_from_ratings(ratings_df)\n",
    "\n",
    "    node2vec = Node2Vec(\n",
    "    graph,\n",
    "    dimensions=embed_dimensions,\n",
    "    walk_length=40,     # Longer walks for richer context.\n",
    "    num_walks=80,      # More walks for better sampling.\n",
    "    p=0.5,\n",
    "    q=10,\n",
    "    weight_key=\"weight\",\n",
    "    workers=1,)\n",
    "\n",
    "    model = node2vec.fit(window=5, min_count=1, batch_words=4)\n",
    "\n",
    "    #extract user nodes and sort them\n",
    "    user_nodes = []\n",
    "\n",
    "    #loop through all nodes and their data in the graph\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        #check if the node_type is 'user'\n",
    "        if \"node_type\" in data and data[\"node_type\"] == \"user\":\n",
    "            user_nodes.append(node)\n",
    "    \n",
    "    #sort the list of user nodes\n",
    "    user_nodes = sorted(user_nodes)\n",
    "\n",
    "    annoy_index = AnnoyIndex(embed_dimensions, metric=\"angular\")\n",
    "    #construct the user_index_map and index_user_map for retrieval with ANN file\n",
    "    user_index_map = {}\n",
    "    index_user_map = {}\n",
    "\n",
    "    for i, user_node in enumerate(user_nodes):\n",
    "        user_vector = model.wv[user_node]\n",
    "        annoy_index.add_item(i, user_vector)\n",
    "        user_index_map[user_node] = i\n",
    "        index_user_map[i] = int(user_node.split(\"_\")[1])\n",
    "\n",
    "    annoy_index.build(num_trees)\n",
    "    annoy_index.save(os.path.join(base_dir, \"global_users.ann\"))\n",
    "    map_info = {\n",
    "        \"user_index_map\": user_index_map,\n",
    "        \"index_user_map\": index_user_map,\n",
    "        \"embed_dimensions\": embed_dimensions\n",
    "    }\n",
    "    with open(os.path.join(base_dir, \"global_map.json\"), \"w\") as f:\n",
    "        json.dump(map_info, f)\n",
    "    \n",
    "    return annoy_index, user_index_map, index_user_map, model\n",
    "    \n",
    "#train embeddings using the training ratings with the directed graph.\n",
    "ann_index, user_index_map, index_user_map, model = create_node2vec_annoy_from_ratings(train_liked_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "96c9ffdc-fda0-42c4-b437-3168709dca28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Precision@10: 0.2702797818053845\n"
     ]
    }
   ],
   "source": [
    "#evaluate the recommendation system using Precision@k.\n",
    "#Annoy returns neighbours, form this set of specified number of neighbours, \n",
    "#we are able to get the top most frequently liked movies as the top-k in Precision@k.\n",
    "#this means Precision@k is a suitable measure, and given the nature of ANN and the associated movies, \n",
    "#is the preferred for performance metrics over just the normal precision rate.\n",
    "\n",
    "def recommend_movies(target_user, neighbour_ids, train_liked_df, top_k=4):\n",
    "    #for the given target user, aggregate liked movies from neighbours,\n",
    "    #remove movies the target user already saw in training,\n",
    "    #rank by frequency (how many neighbours liked the movie),\n",
    "    #and return the top_k recommendations.\n",
    "\n",
    "    #movies the target user already liked in training (to avoid recommending these)\n",
    "    target_seen = set(train_liked_df[train_liked_df['userId'] == target_user][\"movieId\"])\n",
    "\n",
    "    candidate_movies = []\n",
    "    for neighbour in neighbour_ids:\n",
    "        neighbour_movies = set(train_liked_df[train_liked_df['userId'] == neighbour]['movieId'])\n",
    "        candidate_movies.extend(list(neighbour_movies))\n",
    "\n",
    "    #count how often each movie appears among neighbours and filter out already seen movies.\n",
    "    movie_freq = {}\n",
    "    for movie in candidate_movies:\n",
    "        if movie not in target_seen:\n",
    "            movie_freq[movie] = movie_freq.get(movie, 0) + 1\n",
    "\n",
    "    #turn the movie frequency dictionary into a list of (movie, frequency) pairs\n",
    "    movie_freq_list = list(movie_freq.items())\n",
    "    \n",
    "    #sort the list by:\n",
    "    #frequency in descending order (highest first)\n",
    "    #if two movies have the same frequency, sort by movieId in ascending order\n",
    "    def sort_key(item):\n",
    "        movie_id = item[0]\n",
    "        freq = item[1]\n",
    "        return (-freq, movie_id)\n",
    "\n",
    "    sorted_movies = sorted(movie_freq_list, key=sort_key)\n",
    "\n",
    "    #extract only the movie Ids of the top_k items\n",
    "    recommended_movies = []\n",
    "    for i in range(min(top_k, len(sorted_movies))):\n",
    "        movie_id = sorted_movies[i][0]\n",
    "        recommended_movies.append(movie_id)\n",
    "\n",
    "    return recommended_movies\n",
    "\n",
    "#returns overall average precision and a dictionary of per-user precision scores.\n",
    "def evaluate_precision_at_k(ann_index, user_index_map, index_user_map,\n",
    "                            train_liked_df, test_liked_df,\n",
    "                            k_neighbours=3, k_recs=10):\n",
    "\n",
    "    #for each user:\n",
    "    #retrieve the k closest neighbours using the Annoy index.\n",
    "    #aggregate neighbours' liked movies to generate top k recommendations.\n",
    "    #compute precision@K as (# recommended movies that are in the user's held-out liked set) / k_recs.\n",
    "    user_precision = {}\n",
    "    for user_node, target_index in user_index_map.items():\n",
    "        #extract the target user Id from string\n",
    "        target_user = int(user_node.split(\"_\")[1])\n",
    "        \n",
    "        #get k+1 nearest neighbours (target user included), then remove the target itself.\n",
    "\n",
    "        nearest_indices = ann_index.get_nns_by_item(target_index, k_neighbours + 1)\n",
    "\n",
    "        #create a new list to hold the neighbour indices, excluding the target user\n",
    "        filtered_indices = []\n",
    "    \n",
    "        for i in nearest_indices:\n",
    "            if i != target_index:\n",
    "                filtered_indices.append(i)\n",
    "        \n",
    "        #keep only the first k_neighbours\n",
    "        nearest_indices = filtered_indices[:k_neighbours]\n",
    "\n",
    "        neighbour_ids = [index_user_map[i] for i in nearest_indices]\n",
    "\n",
    "        #get recommendations from neighbours\n",
    "        recommended = recommend_movies(target_user, neighbour_ids, train_liked_df, top_k=k_recs)\n",
    "        #compare against the target user's held-out liked movies (test set)\n",
    "        test_liked = set(test_liked_df[test_liked_df['userId'] == target_user]['movieId'])\n",
    "\n",
    "        #convert recommended list to a set\n",
    "        recommended_set = set(recommended)\n",
    "        \n",
    "        #find the intersection (i.e. common movies) between recommended and test_liked\n",
    "        common_movies = recommended_set.intersection(test_liked)\n",
    "        \n",
    "        #count how many recommended movies are actually liked (true positives)\n",
    "        num_correct = len(common_movies)\n",
    "        \n",
    "        #if k_recs is more than 0, calculate precision; otherwise, set it to 0\n",
    "        if k_recs > 0:\n",
    "            precision = num_correct / float(k_recs)\n",
    "        else:\n",
    "            precision = 0\n",
    "\n",
    "        user_precision[target_user] = precision\n",
    "\n",
    "    #check if there are any users in the dictionary\n",
    "    if len(user_precision) > 0:\n",
    "        total_precision = sum(user_precision.values())\n",
    "        num_users = len(user_precision)\n",
    "        overall_precision = total_precision / num_users\n",
    "    else:\n",
    "        overall_precision = 0\n",
    "    return overall_precision, user_precision\n",
    "\n",
    "#run function to evaluate Precision@K\n",
    "overall_prec_at_k, per_user_prec_at_k = evaluate_precision_at_k(\n",
    "    ann_index, user_index_map, index_user_map,\n",
    "    train_liked_df, test_liked_df,\n",
    "    k_neighbours=3, k_recs=1\n",
    ")\n",
    "print(\"Overall Precision@10:\", overall_prec_at_k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
